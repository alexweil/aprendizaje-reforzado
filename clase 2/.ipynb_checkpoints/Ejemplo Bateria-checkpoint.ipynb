{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "states = [\"high\", \"low\"]\n",
    "actions = [\"wait\", \"search\", \"recharge\"]\n",
    "\n",
    "P = {}\n",
    "\n",
    "P[0] = {}\n",
    "P[1] = {}\n",
    "\n",
    "alpha = 0.9\n",
    "beta = 0.9\n",
    "r_wait = 2.0\n",
    "r_search = 4.0\n",
    "\n",
    "# definimos un ambiente discreto con las transiciones según el gráfico\n",
    "\n",
    "P[0][0] = [(1.0, 0, r_wait, False)]\n",
    "P[0][1] = [(alpha, 0, r_search, False),\n",
    "           (1-alpha, 1, r_search, False)]\n",
    "P[0][2] = [(1.0, 0, 0.0, False)]\n",
    "\n",
    "P[1][0] = [(1.0, 0, r_wait, False)]\n",
    "P[1][1] = [(beta, 1, r_search, False), \n",
    "           (1-beta, 0, -3.0, False)]\n",
    "P[1][2] = [(1.0, 0, 0.0, False)]\n",
    "\n",
    "env = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del ejercicio de evaluación de política\n",
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluar una política dado un ambiente y una descripción completa\n",
    "    de la dinámica del ambiente.\n",
    "    \n",
    "    Argumentos:\n",
    "        política: matriz de tamaño [S, A] representando la política.\n",
    "        env: ambiente de OpenAI representadno las probabilidades de transición\n",
    "        del ambiente. \n",
    "        env.P[s][a] es una lista de vectores (probabilidad, próximo_estado, recompensa, done)\n",
    "        env.nS es el número de estados en el ambiente\n",
    "        env.nA es el número de acciones en el ambiente\n",
    "        theta: para la evaluación de la política una vez que la función de valor cambia menos que\n",
    "        theta para todos los estados\n",
    "        discount_factor: factor de descuento gama.\n",
    "        \n",
    "    Retorna:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    # Comenzar con función de valor aleatoria\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Para cada estado realizar un \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Fijarse en las posibles próximas acciones\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # Para cada acción, fijarse en los próximos estados\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calcular el valor esperado\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # Cuál fue el máximo cambio de la función de valor\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Para de evaluar una vez que estamos debajo de un cierto umbral\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Algoritmo de mejora de una política. Evalúa iterativamente y mejora una política \n",
    "    hasta que encuentra la política óptima.\n",
    "    \n",
    "    Args:\n",
    "        env: ambiente de OpenAI.\n",
    "        policy_eval_fn: función de evaluación de política que toma 3 argumentos: policy, env, discount_factor\n",
    "        discount_factor: factor de descuento gama\n",
    "        \n",
    "    Retorna:\n",
    "        Un tuple (policy, V)\n",
    "        A tuple (policy, V). \n",
    "        policy es la política óptima, una matriz de tamaño [S, A] en que cada estado s contiene una distribución de probabilidad \n",
    "        valida sobre el espacio de acciones.\n",
    "        V es la función de valor para la política óptima.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        policy_stable = True\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        for i in range(env.nS):\n",
    "            old_action = np.argmax(policy[i])\n",
    "            action_value_function_list = []\n",
    "            \n",
    "            for j in range(env.nA):\n",
    "                action_value_function = 0\n",
    "                \n",
    "                for  prob, next_state, reward, done in env.P[i][j]:\n",
    "                    action_value_function += prob * (reward + discount_factor * V[next_state])\n",
    "                    \n",
    "                action_value_function_list.append(action_value_function)\n",
    "\n",
    "            best_action = np.argmax(action_value_function_list)\n",
    "            policy[i, :] = 0\n",
    "            policy[i, best_action] = 1\n",
    "                    \n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "                            \n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 0\n",
      "reward: -3.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 0\n",
      "reward: 4.0\n",
      "------------------------------\n",
      "value: [5.30416578 4.42916616]\n",
      "observation: 1\n",
      "reward: 4.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "observation = 0\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    policy, v = policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=0.25)\n",
    "    action = policy[observation].argmax()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    print('value: %s' % v)\n",
    "    print('observation: %s' % observation)\n",
    "    print('reward: %s' % reward)\n",
    "    print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
